# -*- coding: utf-8 -*-
"""ML_News.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ful2p1mI4ksDVtGyeYIpb1GShJZ2BXW4
"""

#importing necessary libraries
import requests
from bs4 import BeautifulSoup
import csv
import time
from datetime import datetime, timedelta

url="https://www.bbc.com/news"
response=requests.get(url)
soup=BeautifulSoup(response.text,"html.parser")
print(soup.prettify()[:6000])

news = []
news_sections = soup.find_all('article', class_='sc-9636e898-0 dYtsiK')

print(f"Found {len(news_sections)} news_sections.")  # Debugging: Print number of articles found


for section in news_sections:
    articles = section.find_all('div', class_='sc-c6f6255e-0 eGcloy')
    for article in articles:
        title_element = article.find('h2', class_='sc-8ea7699c-3 dhclWg')  # extract headlines
        title = title_element.text.strip() if title_element else "NA"

        # Assuming 'Place' is represented by the category
        category_element = article.find('span', class_='sc-6fba5bd4-2 bHkTZK')  # extract categories
        place = category_element.text.strip() if category_element else "NA"

        date_element = article.find('span', class_='sc-6fba5bd4-1 efYorw')  # extract time
        date = date_element.text.strip() if date_element else 'NA'

        # Parse relative time format
        if date != 'NA':
            if 'hrs ago' in date:
                hrs_ago = int(date.split(' ')[0])
                real_time = datetime.now() - timedelta(hours=hrs_ago)
                real_time = real_time.strftime("%H:%M:%S")
            elif 'mins ago' in date:
                mins_ago = int(date.split(' ')[0])
                real_time = datetime.now() - timedelta(minutes=mins_ago)
                real_time = real_time.strftime("%H:%M:%S")
            else:
                real_time = 'NA'
        else:
            real_time = 'NA'

        # Assuming 'Feeder' is a placeholder for the source
        feeder = "BBC News"

        # Extracting the link
        link_element = article.find('a', href=True)
        link = link_element['href'] if link_element else "NA"
        if link != "NA" and not link.startswith("http"):
            link = "https://www.bbc.com" + link

        # Extracting the description
        description_element = article.find('p', class_='sc-b8778340-4 kYtujW')
        description = description_element.text.strip() if description_element else "NA"

        news.append({
            'Title': title,
            'Place': place,
            'Time': real_time,
            'Feeder': feeder,
            'Link': link,
            'Description': description
        })

for news_item in news:
    print(news_item)

import pandas as pd
df = pd.DataFrame(news)
df.to_csv("bbcnews.csv", index=False, encoding="utf-8")
print("Scraping completed! Data saved to 'bbcnews.csv'.")

df = pd.read_csv('bbcnews.csv')
df.head()

df.shape

df['Title'].fillna("No Title Available", inplace=True)
df['Place'].fillna("Unknown Category", inplace=True)
df.drop_duplicates(inplace=True)
print(f"Shape after cleaning: {df.shape}")

# Save cleaned data
df.to_csv('bbcnews_cleaned.csv', index=False, encoding='utf-8')
print("Cleaned data saved to 'bbcnews_cleaned.csv'.")

print(df.head())

print(df.info())

print(df.isnull().sum())

#TF-IDF similarity
def find_similar_headlines(keyword, tfidf_matrix, vectorizer, df, top_n=5):
    keyword_vec = vectorizer.transform([keyword])
    cosine_sim = cosine_similarity(keyword_vec, tfidf_matrix)
    similar_indices = cosine_sim.argsort()[0][-top_n:][::-1]
    return df.iloc[similar_indices]

# Load cleaned data
df = pd.read_csv('bbcnews_cleaned.csv')

df.head()

# TF-IDF Vectorization
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(df['Title'])

df['Description'].fillna("No Description Available", inplace=True)
df['Title'].fillna("No Title Available", inplace=True)
df['Place'].fillna("Unknown Location", inplace=True)

category_counts = df['Place'].value_counts()
print("Category-wise counts:\n", category_counts)

top_news = df[df['Description'] != 'No Description Available'].head(10)
print(top_news)

print(df.isnull().sum())

df2=df.to_csv('structured_bbcnews.csv', index=False)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# Load your data
df = pd.read_csv('structured_bbcnews.csv')

# Preprocessing (optional: cleaning and stemming/lemmatization)
# Assuming 'Title' column is clean
headlines = df['Title']

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(headlines)

# Function to find the most similar headlines to the entered keyword
def find_similar_headlines(keyword, tfidf_matrix, vectorizer, top_n=5):
    keyword_vec = vectorizer.transform([keyword])
    cosine_sim = cosine_similarity(keyword_vec, tfidf_matrix)
    similar_indices = cosine_sim.argsort()[0][-top_n:][::-1]
    return df.iloc[similar_indices]

# Example: Find headlines similar to 'Facebook'
keyword = 'Facebook'
similar_headlines = find_similar_headlines(keyword, tfidf_matrix, vectorizer)
print(similar_headlines[['Title', 'Link']])

import pickle

# Components to save
data_to_save = {
    'vectorizer': vectorizer,
    'tfidf_matrix': tfidf_matrix,
    'dataframe': df
}

# Save to a pickle file
with open('headline_similarity_model.pkl', 'wb') as f:
    pickle.dump(data_to_save, f)

print("Model and data saved to 'headline_similarity_model.pkl'")

from google.colab import files
files.download('headline_similarity_model.pkl')

